{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1 Text Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Declare functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Kio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "import gzip # to install: pip install npc-gzip\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rank_bm25 import BM25Okapi\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to calculate the NCD and return the indexes of the top k smallest distances\n",
    "def compressed_search(query, text, k):\n",
    "    x1 = query[0] if isinstance(query, tuple) else query\n",
    "    Cx1 = len(gzip.compress(x1.encode()))\n",
    "    distance_from_x1 = []\n",
    "    # 2 cases will pass through this if statement: text is a dictionary or a tuple\n",
    "    if isinstance(text, dict):\n",
    "        # If text is a dictionary, iterate through its values\n",
    "        for x2 in text.values():\n",
    "            Cx2 = len(gzip.compress(x2.encode()))\n",
    "            x1x2 = \" \".join([x1, x2])\n",
    "            Cx1x2 = len(gzip.compress(x1x2.encode()))\n",
    "            ncd = (Cx1x2 - min(Cx1, Cx2)) / max(Cx1, Cx2)\n",
    "            distance_from_x1.append(ncd)\n",
    "    elif isinstance(text, list):\n",
    "        for (x2,_) in text:\n",
    "            Cx2 = len(gzip.compress(x2.encode()))\n",
    "            x1x2 = \" \".join([x1, x2])\n",
    "            Cx1x2 = len(gzip.compress(x1x2.encode()))\n",
    "            ncd = (Cx1x2 - min(Cx1, Cx2)) / max(Cx1, Cx2)\n",
    "            distance_from_x1.append(ncd)\n",
    "    return np.argsort(np.array(distance_from_x1))[:k] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cranfield Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "with open('cranfield/cranfield.dat', 'r') as f:\n",
    "    cranfield_lines = f.readlines()\n",
    "    idx = 1\n",
    "    cranfield_lines_idx = []\n",
    "    for line in cranfield_lines:\n",
    "        cranfield_lines_idx.append((line,idx))\n",
    "        idx += 1\n",
    "\n",
    "# Load the qrels\n",
    "cranfield_qrels = {}\n",
    "with open('cranfield/cranfield-qrels.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        numbers = [int(n) for n in line.split()]\n",
    "        key = numbers[0]\n",
    "        if key not in cranfield_qrels:\n",
    "            cranfield_qrels[key] = []\n",
    "        cranfield_qrels[key].append(numbers[1:])\n",
    "\n",
    "# Load the queries\n",
    "with open('cranfield/cranfield-queries.txt', 'r') as f:\n",
    "    cranfield_queries = f.readlines()\n",
    "    idx = 1\n",
    "    cranfield_queries_idx = []\n",
    "    for line in cranfield_queries:\n",
    "        cranfield_queries_idx.append((line,idx))\n",
    "        idx += 1\n",
    "\n",
    "# Load the stopwords\n",
    "with open('stopwords.txt', 'r') as f:\n",
    "    stopwords = f.readlines()\n",
    "    stopwords = [word.strip() for word in stopwords]\n",
    "\n",
    "def preprocess(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove stopwords\n",
    "    filtered_tokens = [word for word in tokens if word not in stopwords]\n",
    "    return filtered_tokens\n",
    "\n",
    "def calc_ndcg(scored_result, cranfield_qrels, k):\n",
    "    # Calculate the NDCG\n",
    "    ndcg_result = []\n",
    "    for key in scored_result:\n",
    "        # Calculate the DCG\n",
    "        dcg = 0\n",
    "        for i in range(len(scored_result[key])):\n",
    "            if i == 0:\n",
    "                dcg += scored_result[key][i][1] # first element is not divided by log2\n",
    "            else:\n",
    "                dcg += scored_result[key][i][1] / np.log2(i+1) # i+1 because the index starts from 0\n",
    "        # Calculate the IDCG        \n",
    "        idcg = 0\n",
    "        length = len(cranfield_qrels[key]) if len(cranfield_qrels[key]) < k else k # if the length of qrels is less than k, use the length of qrels\n",
    "        sorted_cranfield_qrels = sorted(cranfield_qrels[key], key=lambda x: x[1], reverse=True) # sort the qrels by the score for IDCG\n",
    "        for i in range(length): # calculate the IDCG\n",
    "            if i == 0:\n",
    "                idcg += sorted_cranfield_qrels[i][1]\n",
    "            else:\n",
    "                idcg += sorted_cranfield_qrels[i][1] / np.log2(i+1)\n",
    "        ndcg_result.append(dcg/idcg) # calculate the NDCG and append it to the list\n",
    "    return np.mean(np.array(ndcg_result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test compression method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare K\n",
    "k = 10\n",
    "# Execute the test and store the result\n",
    "test_result = {}\n",
    "for i in range(len(cranfield_queries_idx)):\n",
    "    key = i+1\n",
    "    if key not in test_result:\n",
    "        test_result[key] = []\n",
    "    # passing each query to the search function with the whole text and k\n",
    "    test_result[key].append(compressed_search(cranfield_queries_idx[i], cranfield_lines_idx, k))\n",
    "\n",
    "# Score the result by matching the result with the qrels\n",
    "scored_result = {}\n",
    "for key in test_result:\n",
    "    if key not in scored_result:\n",
    "        scored_result[key] = []\n",
    "    # test_result[key][0] is the list of indexes of compressed_search result\n",
    "    for i in range(len(test_result[key][0])): \n",
    "        for j in range(len(cranfield_qrels[key])):\n",
    "            if test_result[key][0][i] == cranfield_qrels[key][j][0]:\n",
    "                scored_result[key].append((i+1, cranfield_qrels[key][j][1]))\n",
    "                break\n",
    "        if len(scored_result[key]) < i+1:\n",
    "            scored_result[key].append((i+1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nDCG@10 of the compressed search is  0.02736\n"
     ]
    }
   ],
   "source": [
    "cran_compress_ndcg = calc_ndcg(scored_result, cranfield_qrels, k)\n",
    "print(f\"The nDCG@{k} of the compressed search is \", format(cran_compress_ndcg, '.5f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare K\n",
    "k = 10\n",
    "# Tokenize the corpus and remove the stopwords\n",
    "tokenized_corpus = [preprocess(line) for line, _ in cranfield_lines_idx]\n",
    "\n",
    "# Train the model\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# Execute the test and store the result\n",
    "test_result = {}\n",
    "for i in range(len(cranfield_queries_idx)):\n",
    "    key = i+1\n",
    "    if key not in test_result:\n",
    "        test_result[key] = []\n",
    "    # tokenize the query and get the top k result\n",
    "    tokenized_query = preprocess(cranfield_queries_idx[i][0])\n",
    "    # append the result to the test_result\n",
    "    test_result[key].append(np.argsort(bm25.get_scores(tokenized_query))[::-1][:k]) # [::-1] to sort in descending order\n",
    "\n",
    "# Score the result by matching the result with the qrels\n",
    "scored_result = {}\n",
    "for key in test_result:\n",
    "    if key not in scored_result:\n",
    "        scored_result[key] = []\n",
    "    # test_result[key][0] is the list of indexes of compressed_classification result\n",
    "    for i in range(len(test_result[key][0])): \n",
    "        for j in range(len(cranfield_qrels[key])):\n",
    "            if test_result[key][0][i] == cranfield_qrels[key][j][0]:\n",
    "                scored_result[key].append((i+1, cranfield_qrels[key][j][1]))\n",
    "                break\n",
    "        if len(scored_result[key]) < i+1:\n",
    "            scored_result[key].append((i+1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nDCG@10 of the BM25 search is  0.33176\n"
     ]
    }
   ],
   "source": [
    "cran_bm25_ndcg = calc_ndcg(scored_result, cranfield_qrels, k)\n",
    "print(f\"The nDCG@{k} of the BM25 search is \", format(cran_bm25_ndcg, '.5f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CISI Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_documents ():\n",
    "    f = open (\"CISI/CISI.ALL\")\n",
    "    merged = \" \"\n",
    "    # the string variable merged keeps the result of merging the field identifier with its content\n",
    "    \n",
    "    for a_line in f.readlines ():\n",
    "        if a_line.startswith (\".\"):\n",
    "            merged += \"\\n\" + a_line.strip ()\n",
    "        else:\n",
    "            merged += \" \" + a_line.strip ()\n",
    "    # updates the merged variable using a for-loop\n",
    "    \n",
    "    documents = {}\n",
    "    \n",
    "    content = \"\"\n",
    "    doc_id = \"\"\n",
    "    # each entry in the dictioanry contains key = doc_id and value = content\n",
    "    \n",
    "    for a_line in merged.split (\"\\n\"):\n",
    "        if a_line.startswith (\".I\"):\n",
    "            doc_id = a_line.split (\" \") [1].strip()\n",
    "        elif a_line.startswith (\".X\"):\n",
    "            documents[doc_id] = content\n",
    "            content = \"\"\n",
    "            doc_id = \"\"\n",
    "        else:\n",
    "            content += a_line.strip ()[3:] + \" \"\n",
    "    f.close ()\n",
    "    return documents\n",
    "\n",
    "\n",
    "documents = read_documents ()\n",
    "\n",
    "with open('CISI/CISI.QRY') as f:\n",
    "    lines = \"\"\n",
    "    for l in f.readlines():\n",
    "        lines += \"\\n\" + l.strip() if l.startswith(\".\") else \" \" + l.strip()\n",
    "    lines = lines.lstrip(\"\\n\").split(\"\\n\")\n",
    "    \n",
    "qry_set = {}\n",
    "qry_id = \"\"\n",
    "for l in lines:\n",
    "    if l.startswith(\".I\"):\n",
    "        qry_id = l.split(\" \")[1].strip()\n",
    "    elif l.startswith(\".W\"):\n",
    "        qry_set[qry_id] = l.strip()[3:]\n",
    "        qry_id = \"\"\n",
    "\n",
    "rel_set = {}\n",
    "with open('CISI/CISI.REL') as f:\n",
    "    for l in f.readlines():\n",
    "        qry_id = l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[0]\n",
    "        doc_id = l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[-1]\n",
    "        if qry_id in rel_set:\n",
    "            rel_set[qry_id].append(doc_id)\n",
    "        else:\n",
    "            rel_set[qry_id] = []\n",
    "            rel_set[qry_id].append(doc_id)\n",
    "\n",
    "def calc_map(scored_result):\n",
    "    # Calculate the MAP\n",
    "    map_result = []\n",
    "    for key in scored_result.keys():\n",
    "        temp = 0\n",
    "        for i in range(len(scored_result[key])):\n",
    "            if scored_result[key][i][1] == 1:\n",
    "                temp += 1\n",
    "        map_result.append(temp/(len(scored_result[key])))\n",
    "    return np.mean(np.array(map_result))\n",
    "\n",
    "def calc_mrr(scored_result):\n",
    "    # Calculate the MRR\n",
    "    mrr_result = []\n",
    "    for key in scored_result.keys():\n",
    "        temp = 0\n",
    "        for i in range(len(scored_result[key])):\n",
    "            if scored_result[key][i][1] == 1:\n",
    "                temp = 1/(i+1)\n",
    "        mrr_result.append(temp)\n",
    "    return np.mean(np.array(mrr_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test compression method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare K\n",
    "k = 10\n",
    "# Execute the test and store the result\n",
    "test_result = {}\n",
    "for key in qry_set.keys():\n",
    "    if key not in test_result:\n",
    "        test_result[key] = []\n",
    "    # passing each query to the search function with the whole text and k\n",
    "    test_result[key].append(compressed_search((qry_set[key],0), documents, k))\n",
    "\n",
    "# Score the result by matching the result with the qrels\n",
    "scored_result = {}\n",
    "for key in test_result.keys():\n",
    "    if key in rel_set.keys():\n",
    "        if key not in scored_result.keys():\n",
    "            scored_result[key] = []\n",
    "        for test in test_result[key]:\n",
    "            for i in test:\n",
    "                # i+1 because the index starts from 0\n",
    "                temp = str(i+1)\n",
    "                if temp in rel_set[key]:\n",
    "                    scored_result[key].append((i+1, 1))\n",
    "                else:\n",
    "                    scored_result[key].append((i+1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAP of the compressed search is  0.07763\n",
      "The MRR of the compressed search is  0.13421\n"
     ]
    }
   ],
   "source": [
    "cisi_compress_map = calc_map(scored_result)\n",
    "cisi_mean_reciprocal_rank = calc_mrr(scored_result)\n",
    "print(f\"The MAP of the compressed search is \", format(cisi_compress_map, '.5f'))\n",
    "print(f\"The MRR of the compressed search is \", format(cisi_mean_reciprocal_rank, '.5f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare K\n",
    "k = 3\n",
    "\n",
    "# Tokenize the corpus and remove the stopwords\n",
    "tokenized_corpus = [preprocess(line) for line in documents.values()]\n",
    "\n",
    "# Train the model\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# Execute the test and store the result\n",
    "test_result = {}\n",
    "for key in qry_set.keys():\n",
    "    if key not in test_result:\n",
    "        test_result[key] = []\n",
    "    # tokenize the query and get the top k result\n",
    "    tokenized_query = preprocess(qry_set[key])\n",
    "    # append the result to the test_result\n",
    "    test_result[key].append(np.argsort(bm25.get_scores(tokenized_query))[::-1][:k]) # [::-1] to sort in descending order\n",
    "\n",
    "\n",
    "# Score the result by matching the result with the qrels\n",
    "scored_result = {}\n",
    "for key in test_result.keys():\n",
    "    if key in rel_set.keys():\n",
    "        if key not in scored_result.keys():\n",
    "            scored_result[key] = []\n",
    "        for test in test_result[key]:\n",
    "            for i in test:\n",
    "                temp = str(i+1)\n",
    "                if temp in rel_set[key]:\n",
    "                    scored_result[key].append((i+1, 1))\n",
    "                else:\n",
    "                    scored_result[key].append((i+1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAP of the BM25 search is  0.41228\n",
      "The MRR of the BM25 search is  0.35307\n"
     ]
    }
   ],
   "source": [
    "cisi_bm25_map = calc_map(scored_result)\n",
    "cisi_bm25_mean_reciprocal_rank = calc_mrr(scored_result)\n",
    "print(f\"The MAP of the BM25 search is \", format(cisi_bm25_map, '.5f'))\n",
    "print(f\"The MRR of the BM25 search is \", format(cisi_bm25_mean_reciprocal_rank, '.5f'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
